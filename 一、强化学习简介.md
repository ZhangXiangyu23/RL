## 一、强化学习简介

#### 1.基本概念

强化学习：通过从交互中学习来实现目标的计算方法。

交互过程：在每一步t，智能体：获得观察O_t,获得奖励R_t,执行行动A_t，环境：获得行动A_t,给出观察O_{t+1},给出奖励R_{t+1}

历史(History)：是观察、奖励、行动的序列，即一直到时间t为止的所有可观测变量。

状态（State）：是一种用于确定接下来会发生的事情（A,R,O）,状态是关于历史的函数。

状态通常是整个环境的， 观察可以理解为是状态的一部分，仅仅是agent可以观察到的那一部分。

策略(Policy)：是学习智能体在特定时间的行为方式。是从状态到行为的映射。

确定性策略：函数表示，随机策略：条件概率表示

奖励(Reward)：立即感知到什么是好的，一般情况下就是一个标量

价值函数（Value function）：长期而言什么是好的

价值函数是对于未来累计奖励的预测，用于评估给定策略下，状态的好坏

环境的模型（Model）：用于模拟环境的行为，预测下一个状态，预测下一个立即奖励（reward）

强化学习智能体的分类

model-based RL:模型可以被环境所知道，agent可以直接利用模型执行下一步的动作，而无需与实际环境进行交互学习。

比如：围棋、迷宫

model_free RL：真正意义上的强化学习，环境是黑箱

比如Atari游戏，需要大量的采样

基于价值：没有策略（隐含）、价值函数

基于策略：策略、没有价值函数

Actor-Critic：策略、价值函数



#### 2.总结

1.机器学习类型分为预测类和决策类，预测分为有监督学习（根据数据预测所需输出）和无监督学习（生成数据实例），强化学习（在动态环境中采取行动）属于决策类。

2.强化学习是通过从交互学习来实现目标的计算方法。有感知、行动、目标三个方面。

3.在每一步t,智能体获得观察O_t，获得奖励R_t，执行行动A_t。环境获得行动A_t，给出观察O_{t+1}，给出奖励R_{t+1}。

4.强化学习系统要素为：历史、状态、策略、奖励和价值函数。

（1）历史是观察、行动和奖励的序列。

（2）状态是一种用于确定接下来会发生的事情（行动、观察、奖励）的信息。

（3）策略是状态到行动的映射，分为确定性策略和随机策略。

（4）奖励是对未来累积奖励的预测。

（5）价值函数是对未来累积奖励的预测，用于评估在给定的策略下，状态的好坏。

5.环境的模型用于模拟环境的行为，包括预测下一个状态和下一个（立即）奖励。

6.强化学习智能体分类：

（1）基于模型的强化学习。策略（和/或）价值函数，环境模型。如迷宫游戏、迷宫。

（2）模型无关的强化学习。策略（和/或）价值函数，没有环境模型。如Atari游戏的通用攻略